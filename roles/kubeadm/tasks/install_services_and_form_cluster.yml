---
- name: Load kernel modules
  lineinfile:
    path: /etc/modules-load.d/k8s.conf
    line: "{{ item }}"
    create: true
  loop:
    - overlay
    - br_netfilter
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Modprobe overlay and br_netfilter
  modprobe:
    name: "{{ item }}"
    state: present
  loop:
    - overlay
    - br_netfilter
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Configure sysctl parameters
  blockinfile:
    path: /etc/sysctl.d/k8s.conf
    create: true
    block: |
      net.bridge.bridge-nf-call-iptables  = 1
      net.bridge.bridge-nf-call-ip6tables = 1
      net.ipv4.ip_forward                 = 1
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Apply sysctl params without reboot
  command: sysctl --system
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Disable swap
  shell: "sudo swapoff -a"
  ignore_errors: true
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Add cron job to disable swap on reboot
  shell: '(crontab -l 2>/dev/null; echo "@reboot /sbin/swapoff -a") | crontab - || true'
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Set OS and k8s VERSION variables
  set_fact:
    OS: "{{ ubuntu_version }}"
    VERSION: "{{ k8s_version }}"
  tags: [add_new_master_nodes,add_new_worker_nodes]

#new install for crio
- name: Update apt repository cache
  apt:
    update_cache: yes
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Install required dependencies
  apt:
    name: "{{ item }}"
    state: present
    update_cache: true 
  loop:
    - software-properties-common
    - gpg
    - curl
    - apt-transport-https
    - ca-certificates
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Add GPG key for CRI-O repository
  ansible.builtin.shell: |
    curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key |
    gpg --dearmor -o /etc/apt/keyrings/cri-o-apt-keyring.gpg
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Add CRI-O repository to apt sources
  ansible.builtin.shell: |
    echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/ /" |
    tee /etc/apt/sources.list.d/cri-o.list
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Update apt repository cache again
  apt:
    update_cache: yes
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Install CRI-O package
  apt:
    name: cri-o
    state: present
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Reload systemd and enable/start CRI-O service
  systemd:
    name: crio
    enabled: yes
    state: started
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Install crictl (CLI utility)
  ansible.builtin.shell: |
    VERSION="{{ crictl_version }}"
    wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz
    sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin
    rm -f crictl-$VERSION-linux-amd64.tar.gz
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Reload systemd configurations
  systemd:
    name: systemd-journald
    state: restarted
  tags: [add_new_master_nodes,add_new_worker_nodes]
  run_once: true

# #containerd install  - if preffered over crio then uncomment this and comment crio 
# - name: Download and unpack Containerd
#   get_url:
#     url: "https://github.com/containerd/containerd/releases/download/v{{ containerd_version }}/containerd-{{ containerd_version }}-linux-amd64.tar.gz"
#     dest: "/tmp/containerd.tar.gz"
#   tags: [add_new_master_nodes,add_new_worker_nodes]

# - name: Extract Containerd
#   unarchive:
#     src: "/tmp/containerd.tar.gz"
#     dest: "/usr/local"
#     remote_src: yes
#   tags: [add_new_master_nodes,add_new_worker_nodes]

# - name: Download runc
#   get_url:
#     url: "https://github.com/opencontainers/runc/releases/download/v{{ runc_version }}/runc.amd64"
#     dest: "/usr/local/sbin/runc"
#     mode: "0755"
#   tags: [add_new_master_nodes,add_new_worker_nodes]

# - name: Create directory for CNI Plugins
#   file:
#     path: "/opt/cni/bin"
#     state: directory

# - name: Download CNI Plugins
#   get_url:
#     url: "https://github.com/containernetworking/plugins/releases/download/v{{ cni_plugins_version }}/cni-plugins-linux-amd64-v{{ cni_plugins_version }}.tgz"
#     dest: "/tmp/cni-plugins.tgz"
#   tags: [add_new_master_nodes,add_new_worker_nodes]

# - name: Extract CNI Plugins
#   unarchive:
#     src: "/tmp/cni-plugins.tgz"
#     dest: "/opt/cni/bin"
#     remote_src: yes
#   tags: [add_new_master_nodes,add_new_worker_nodes]

# - name: Create Containerd configuration directory
#   file:
#     path: "/etc/containerd"
#     state: directory
#   tags: [add_new_master_nodes,add_new_worker_nodes]

# # #don't create this file because it messes kubelet config
# # - name: Create empty config file 
# #   file:
# #     path: /etc/containerd/config.toml
# #     state: touch

# # - name: Generate default Containerd configuration
# #   command: "containerd config default > /etc/containerd/config.toml"
# #   tags: [add_new_master_nodes,add_new_worker_nodes]

# # - name: Enable SystemdCgroup in Containerd configuration
# #   replace:
# #     path: "/etc/containerd/config.toml"
# #     regexp: "systemd_cgroup = false"
# #     replace: "systemd_cgroup = true"
# #   tags: [add_new_master_nodes,add_new_worker_nodes]

# - name: Download Containerd systemd service file
#   get_url:
#     url: "https://raw.githubusercontent.com/containerd/containerd/main/containerd.service"
#     dest: "/etc/systemd/system/containerd.service"
#   tags: [add_new_master_nodes,add_new_worker_nodes]

# - name: Reload systemd daemon
#   systemd:
#     daemon_reload: yes
#   tags: [add_new_master_nodes,add_new_worker_nodes]

# - name: Start and enable Containerd service
#   service:
#     name: containerd
#     state: started
#     enabled: yes
#   tags: [add_new_master_nodes,add_new_worker_nodes]

#main kubeadm kubernetes section below
- name: Install apt-transport-https, ca-certificates, curl
  apt:
    name: "{{ item }}"
    state: present
    update_cache: true
  loop:
    - apt-transport-https
    - ca-certificates
    - curl
    - jq
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Check if GPG key file already exists
  stat:
    path: /etc/apt/keyrings/kubernetes-apt-keyring-v{{ k8s_version }}.gpg
  register: gpg_key_file
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Download GPG key for Kubernetes APT repository if it doesn't exist
  shell: curl -fsSL https://pkgs.k8s.io/core:/stable:/v{{ k8s_version }}/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring-v{{ k8s_version }}.gpg
  when: not gpg_key_file.stat.exists
  tags: [add_new_master_nodes, add_new_worker_nodes]

- name: Add Kubernetes APT repository
  lineinfile:
    path: /etc/apt/sources.list.d/kubernetes.list
    line: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring-v{{k8s_version}}.gpg] https://pkgs.k8s.io/core:/stable:/v{{k8s_version}}/deb/ /"
    create: true
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Update APT repositories
  apt:
    update_cache: yes
  tags: [add_new_master_nodes,add_new_worker_nodes]

#install kubernetes tools
- name: Install kubelet, kubectl, and kubeadm
  apt:
    name: "{{ item }}"
    state: present
  loop:
    - "kubelet={{ k8s_packages_version }}"
    - "kubectl={{ k8s_packages_version }}"
    - "kubeadm={{ k8s_packages_version }}"
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Add hold to kubernetes packages to prevent accidental update
  dpkg_selections:
    name: "{{ item }}"
    selection: hold
  loop:
    - kubelet
    - kubectl
    - kubeadm
  tags: [add_new_master_nodes,add_new_worker_nodes]

#bash completion for kubectl on control plane nodes
- name: Install bash-completion 
  apt:
    name: bash-completion 
    state: present
  when: inventory_hostname in groups['master_nodes'] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]

- name: Set the kubectl completion script
  shell: |
    kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null
  when: inventory_hostname in groups['master_nodes'] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]

- name: Add kubectl alias
  shell: |
    echo 'alias k=kubectl' >>~/.bashrc
  when: inventory_hostname in groups['master_nodes'] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]

- name: Enable the alias
  command: echo 'complete -o default -F __start_kubectl k' >>~/.bashrc
  when: inventory_hostname in groups['master_nodes'] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]

- name: Get local IP
  shell: ip --json a s | jq -r '.[] | if .ifname == "{{network_interface}}" then .addr_info[] | if .family == "inet" then .local else empty end else empty end'
  register: local_ip
  tags: [add_new_master_nodes,add_new_worker_nodes]

- name: Debug kubeadm certificate join command
  debug:
    var: local_ip.stdout

- name: Add node IP to KUBELET_EXTRA_ARGS
  lineinfile:
    path: /etc/default/kubelet
    line: "KUBELET_EXTRA_ARGS=--node-ip={{ local_ip.stdout }}"
    create: true
  tags: [add_new_master_nodes,add_new_worker_nodes]

#configure kube-vip LB for control plane nodes
- name: Create directory for kube-vip manifest if it doesn't exists
  file:
    path: "/etc/kubernetes/manifests/"
    state: directory
  when: inventory_hostname in groups['master_nodes'] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]

#workaround for kube-vip from 1.29 kubernetes version https://github.com/kube-vip/kube-vip/issues/938#issuecomment-2348506673
- name: Copy kube-vip YAML manifest for initial master node - change from 1.29 kubernetes
  template:
    src: kube-vip-initial-master-node.j2
    dest: /etc/kubernetes/manifests/kube-vip.yaml
    mode: 0600
  when: inventory_hostname in groups['master_nodes'][0]
  tags: [add_new_master_nodes]

- name: Copy kube-vip YAML manifest for all other master nodes
  template:
    src: kube-vip-other-master-nodes.j2
    dest: /etc/kubernetes/manifests/kube-vip.yaml
    mode: 0600
  when: inventory_hostname in groups['master_nodes'][1:] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]

#for cri-socket containerd or cri-o use one of this paths: unix:///var/run/containerd/containerd.sock, unix:///var/run/crio/crio.sock"
- name: K8S with kubeadm init on first master node
  shell: kubeadm init --control-plane-endpoint={{ K8S_LB_IP }}:{{ K8S_LB_PORT }} --cri-socket unix:///var/run/crio/crio.sock --upload-certs --apiserver-cert-extra-sans={{ K8S_LB_IP }},{{K8S_LB_NAME}},{{ NODENAME_MASTER1 }},{{NODENAME_MASTER2}},{{NODENAME_MASTER3}} --pod-network-cidr={{ POD_CIDR }} --node-name {{ NODENAME_MASTER1 }} --ignore-preflight-errors Swap --v=5
  register: kubeadm_init
  when: inventory_hostname in groups['master_nodes'][0]

#check init output in ansible stdout
- ansible.builtin.debug:
    var: kubeadm_init
  when: inventory_hostname in groups['master_nodes'][0]

- name: Create ~/.kube directory
  file:
    path: "$HOME/.kube/"
    state: directory
    owner: "{{ ansible_user_id }}"
    group: "{{ ansible_user_id }}"
    mode: 0755
  when: inventory_hostname in groups['master_nodes'] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]

- stat:
    path: "$HOME/.kube/config"
  register: p
  when: inventory_hostname in groups['master_nodes'] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes] 

- name: Create config file if it doesn't exist
  file:
    path: "$HOME/.kube/config"
    owner: "{{ ansible_user_id }}"
    group: "{{ ansible_user_id }}"
    mode: 0600
    state: '{{ "file" if  p.stat.exists else "touch" }}'
  when: inventory_hostname in groups['master_nodes'] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]

- name: Copy admin.conf to ~/.kube/config on first control plane 
  command: cp -a /etc/kubernetes/admin.conf $HOME/.kube/config
  when: inventory_hostname in groups['master_nodes'][0] 

#install CNI calico after initialization of first control plane, change CIDR if default one was changed during kubeadm init command!
#https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart
- name: Download Calico Tigera Operator manifest
  command: "kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/tigera-operator.yaml"
  when: inventory_hostname in groups['master_nodes'][0]
  run_once: true

- name: Download Calico Custom Resources manifest
  command: "curl https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/custom-resources.yaml -O"
  args:
    chdir: "/var/tmp"
  when: inventory_hostname in groups['master_nodes'][0]
  run_once: true

- name: Replace Calico CIDR with the one provided in the kubeadm init
  ansible.builtin.replace:
    path: "/var/tmp/custom-resources.yaml"
    regexp: '^(.*cidr:\s*)192\.168\.0\.0/16$'
    replace: '\g<1>{{ POD_CIDR }}'
  when: inventory_hostname in groups['master_nodes'][0]

- name: Apply Calico Custom Resources
  command: "kubectl create -f /var/tmp/custom-resources.yaml"
  when: inventory_hostname in groups['master_nodes'][0]

#add metrics server, so we can run commands like kubectl top node, kubectl top pod
- name: Install Metrics Server
  shell: kubectl apply -f https://raw.githubusercontent.com/techiescamp/kubeadm-scripts/main/manifests/metrics-server.yaml
  when: inventory_hostname in groups['master_nodes'][0]

#add other master and worker nodes to form HA cluster
- name: Run kubeadm join command and save it into variable, also trim whitespaces if they exist
  shell: |
    kubeadm token create --print-join-command | awk '{$1=$1};NF'
  register: kubeadm_join
  when: inventory_hostname in groups['master_nodes'][0]
  tags: [add_new_master_nodes,add_new_worker_nodes]

- set_fact:
    kubeadm_join: "{{ kubeadm_join.stdout_lines[0] }}"
    cacheable: true
  when: inventory_hostname in groups['master_nodes'][0]
  tags: [add_new_master_nodes,add_new_worker_nodes]

# - name: Debug kubeadm join command
#   debug:
#     var: hostvars[groups['master_nodes'][0]]['kubeadm_join']

- name: Upload certs and register certificate key into variable, also trim whitespaces if they exist
  shell: |
    kubeadm init phase upload-certs --upload-certs | awk '{$1=$1};NF'
  register: certificate_key
  when: inventory_hostname in groups['master_nodes'][0]
  tags: [add_new_master_nodes]

- set_fact:
    kubeadm_crt_key: "{{ certificate_key.stdout_lines[2] }}"
    cacheable: true
  when: inventory_hostname in groups['master_nodes'][0]
  tags: [add_new_master_nodes]

# - name: Debug kubeadm certificate join command
#   debug:
#     var: hostvars[groups['master_nodes'][0]]['kubeadm_crt_key']

- name: Create kubeadm join script for other control-plane nodes
  copy:
    content: |
      #!/bin/bash

      PORT=10250

      # Check if the kubelet port is in use
      if netstat -tulpn | grep ":$PORT " >/dev/null; then
       logger "Port $PORT is in use, doing nothing on {{ inventory_hostname }}."
      else
        logger "Port $PORT is not in use, executing join command on {{ inventory_hostname }}."
        {{ hostvars[groups['master_nodes'][0]]['kubeadm_join'] }} --cri-socket unix:///var/run/crio/crio.sock --control-plane --certificate-key {{ hostvars[groups['master_nodes'][0]]['kubeadm_crt_key'] }} 2>&1 | logger
      fi
    dest: /var/tmp/kubeadm_join_master.sh
    mode: '0755'
  when: inventory_hostname in groups['master_nodes'][1:] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]
  ignore_errors: true

- name: Execute kubeadm join script on control plane nodes
  shell: /var/tmp/kubeadm_join_master.sh
  when: inventory_hostname in groups['master_nodes'][1:] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]
  ignore_errors: true

- name: Copy admin.conf to ~/.kube/config on other control planes to be able to run kubectl commands
  command: cp -a /etc/kubernetes/admin.conf $HOME/.kube/config
  when: inventory_hostname in groups['master_nodes'][1:] or inventory_hostname in groups['new_master_nodes']
  tags: [add_new_master_nodes]

# - name: Join other master nodes to the cluster
#   shell: |
#     "{{ hostvars[groups['master_nodes'][0]]['kubeadm_join'] }} --control-plane --certificate-key {{ hostvars[groups['master_nodes'][0]]['kubeadm_crt_key'] }}"
#   when: inventory_hostname in groups['master_nodes'][1:]

- name: Create kubeadm join script for worker nodes
  copy:
    content: |
      #!/bin/bash

      PORT=10250

      # Check if the kubelet port is in use
      if netstat -tulpn | grep ":$PORT " >/dev/null; then
       logger "Port $PORT is in use, doing nothing on {{ inventory_hostname }}."
      else
        logger "Port $PORT is not in use, executing join command on {{ inventory_hostname }}."
        {{ hostvars[groups['master_nodes'][0]]['kubeadm_join']}} --cri-socket unix:///var/run/crio/crio.sock 2>&1 | logger
      fi
    dest: /var/tmp/kubeadm_join_worker.sh
    mode: '0755'
  when: inventory_hostname in groups['worker_nodes'] or inventory_hostname in groups['new_worker_nodes']
  tags: [add_new_worker_nodes]
  ignore_errors: true

- name: Execute kubeadm join script on worker nodes
  shell: /var/tmp/kubeadm_join_worker.sh
  when: inventory_hostname in groups['worker_nodes'] or inventory_hostname in groups['new_worker_nodes']
  tags: [add_new_worker_nodes]
  ignore_errors: true

# - name: Join other worker nodes to the cluster
#   shell: |
#     "{{ hostvars[groups['master_nodes'][0]]['kubeadm_join']}}"
#   when: inventory_hostname in groups['worker_nodes']

- name: wait for pods to come up in calico-system before restarting coredns
  shell: |
    kubectl wait --namespace=calico-system --for=condition=Ready pods --selector k8s-app=calico-node --timeout=540s
  any_errors_fatal: true
  when: inventory_hostname in groups['master_nodes'][0]

#restart coredns just to be sure new cni is applied properly
- name: Restart coredns deployment
  shell: |
    kubectl rollout restart -n kube-system deployment coredns
  when: inventory_hostname in groups['master_nodes'][0]

#add worker role to worker nodes instead of <none>
- name: Add a label to the Kubernetes worker nodes
  shell: kubectl label node {{ item }} node-role.kubernetes.io/worker=worker --overwrite
  with_items: "{{ groups['worker_nodes'] + groups['new_worker_nodes'] }}"
  when: inventory_hostname in groups['master_nodes'][0]
  tags: [add_new_worker_nodes]
